{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Идеальное решение на основе предыдущих экспериментов"},{"metadata":{},"cell_type":"markdown","source":"Из ноутбука удалены лишние строки по анализу данных и прошлых экспериментов\nчтобы уменьшить время выполнения ноутбука да и вообще самому легче ориентироваться в коротком ноутбуке\n\nИспользовать **ImageDataAugmentor** не получилось, постоянно возникали какие-то непонятные ошибки, решил настраивать генерацию руками"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install tensorflow --upgrade\n!pip install -q efficientnet","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\nimport os\nimport sys\nimport zipfile\n\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\n\nimport keras as keras\nimport keras.models\nimport keras.layers\nimport keras.backend\nimport keras.callbacks\n\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras import optimizers\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nfrom keras.layers import *\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n\nfrom tensorflow.python.client import device_lib\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n\nimport PIL\nfrom PIL import ImageOps, ImageFilter\n#увеличим дефолтный размер графиков\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#графики в svg выглядят более четкими\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)\nprint('Keras        :', tf.keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device_list = device_lib.list_local_devices()\ndevice_list_GPU = [x.name for x in device_list if 'GPU' in x.name]\nprint ('GPU подключен') if device_list_GPU else  print('GPU не подключен')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_PATH  = '../input/sf-dl-car-classification/'\nPICTURE_PATH = '/dev/shm/'\nMODELS_PATH = '../input/models/'\nOUTPUT_PATH = '../working/car/'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"os.makedirs(OUTPUT_PATH, exist_ok = True)\n\nRANDOM_SEED = 42\n\nnp.random.seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS               = 6\nBATCH_SIZE           = 16\nLR                   = 1e-3\nVALID_SPLIT          = 0.3\n\nCLASS_NUM            = 10\nIMG_SIZE             = 260\nIMG_CHANNELS         = 3\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n\nUSE_BIAS             = False\nKERNEL_REG           = 'l2'\nDROPOUT_RATE         = 0.25\nSTEP_SIZE            = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA / Анализ данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -d -r '/dev/shm/'\n!rm -d -r '../working/car/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!unzip '../input/sf-dl-car-classification/train.zip' -d /dev/shm/\n#!unzip '../input/sf-dl-car-classification/train.zip' -d '../working/car/'\nprint('Распаковка картинок')\nwith zipfile.ZipFile(INPUT_PATH + 'train.zip',\"r\") as z:\n    z.extractall(PICTURE_PATH)\nprint('Распаковка завершена')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(INPUT_PATH + 'train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"### Stratify Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stratify():\n    st = StratifiedShuffleSplit(n_splits = 2, test_size = VALID_SPLIT, random_state = RANDOM_SEED)\n    X = train_df['Id']\n    y = train_df['Category']\n    for train_index, val_index in st.split(X, y):\n        train_files, valid_files, train_labels, valid_labels = X[train_index], X[val_index], y[train_index], y[val_index]\n        \n    #train_files, valid_files, train_labels, valid_labels = \\\n    #    train_test_split(train_df['Id'], train_df['Category'], \n    #                     test_size = VALID_SPLIT, \n    #                     random_state = RANDOM_SEED, \n    #                    stratify = train_df['Category'])\n\n    train_files = pd.DataFrame(train_files)\n    valid_files = pd.DataFrame(valid_files)\n    train_files['Category'] = train_labels\n    valid_files['Category'] = valid_labels\n\n    print(train_files.shape, valid_files.shape)\n    return train_files, valid_files\n\ntrain_files, valid_files = stratify()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def move_files():\n    for cat in categories.index:\n        os.makedirs(f'{OUTPUT_PATH}train/{str(cat)}')\n        os.makedirs(f'{OUTPUT_PATH}valid/{str(cat)}') \n        \n    count_file = 0\n    for index, row in train_files.iterrows():\n        file_path = 'train/' + str(row['Category']) + '/' + str(row['Id'])\n        shutil.move(PICTURE_PATH + file_path, OUTPUT_PATH + file_path)\n        count_file += 1\n    print(f'move {count_file} train files')\n    \n    count_file = 0\n    for index,row in valid_files.iterrows():\n        source_path = 'train/' + str(row['Category']) + '/' + str(row['Id'])\n        destination_path = 'valid/' + str(row['Category']) + '/' + str(row['Id'])\n        shutil.move(PICTURE_PATH + source_path, OUTPUT_PATH + destination_path)\n        count_file += 1\n    print(f'move {count_file} valid files')\n    \n    !rm -d -r '/dev/shm/'\n    \nmove_files()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = 0\nfor i in range(0, 10):\n    dirr = OUTPUT_PATH + f'train/{i}/'\n    count = len([name for name in os.listdir(dirr) if os.path.isfile(os.path.join(dirr, name))])\n    print(f'{i} - ', count)\n    total += count\nprint(f'total is train {total}')\n\ntotal = 0\nfor i in range(0, 10):\n    dirr = OUTPUT_PATH + f'valid/{i}/'\n    count = len([name for name in os.listdir(dirr) if os.path.isfile(os.path.join(dirr, name))])\n    print(f'{i} - ', count)\n    total += count\nprint(f'total is valid {total}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# для размера 380 и EfficientNetB4 получаем на последнем этапе ошибку 'OOM when allocating tensor ... by allocator GPU_0_bfc'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_rescale = 1. / 255\np_rotation_range = 5\np_zoom_range = 0.1\np_width_shift_range = 0.1\np_height_shift_range = 0.1\np_brightness_range = [0.5, 0.1]\np_shear_range = 0.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale = p_rescale,\n    zoom_range = p_zoom_range,\n    rotation_range = p_rotation_range,\n    width_shift_range = p_width_shift_range,\n    height_shift_range = p_height_shift_range,\n    shear_range = p_shear_range,\n    horizontal_flip = True)\n\nvalid_datagen = ImageDataGenerator(\n    rescale = p_rescale,\n    zoom_range = p_zoom_range,\n    rotation_range = p_rotation_range,\n    width_shift_range = p_width_shift_range,\n    height_shift_range = p_height_shift_range,\n    shear_range = p_shear_range,\n    horizontal_flip = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### datagen"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_generators():\n    train_generator = train_datagen.flow_from_directory(\n        OUTPUT_PATH + 'train/',\n        target_size = (IMG_SIZE, IMG_SIZE),\n        batch_size = BATCH_SIZE,\n        class_mode = 'categorical',\n        shuffle = True, \n        seed = RANDOM_SEED)\n\n    valid_generator = valid_datagen.flow_from_directory(\n        OUTPUT_PATH +'valid/',\n        target_size = (IMG_SIZE, IMG_SIZE),\n        batch_size = BATCH_SIZE,\n        class_mode = 'categorical',\n        shuffle = True, \n        seed = RANDOM_SEED)\n    return train_generator, valid_generator\n\ntrain_generator, valid_generator = create_generators()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Строим модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_model = Xception(weights = 'imagenet', include_top = False, input_shape = input_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape = input_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Для размера 260 используем EfficientNetB2\nbase_model = efn.EfficientNetB2(weights = 'imagenet', include_top = False, input_shape = input_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Замораживаем веса в базовой модели\nbase_model.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    # Устанавливаем новую \"голову\" (head)\n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D()) # объединяем все признаки в единый вектор \n\n    model.add(Dense(IMG_SIZE, use_bias = USE_BIAS, kernel_regularizer = KERNEL_REG, activation = 'relu'))\n    #model.add(BatchNormalization())\n    model.add(Dropout(DROPOUT_RATE))\n    model.add(Dense(CLASS_NUM, activation = 'softmax'))\n    model.summary()\n    \n    return model\n\ndef create_callbacks():\n    checkpoint = ModelCheckpoint('best_model.hdf5', monitor = 'val_accuracy', verbose = 1, mode = 'max', save_best_only = True)\n    earlystop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, verbose = 1, patience = 3, restore_best_weights = True)    \n    def scheduler(epoch):\n        return LR * math.pow(math.exp(-0.1), math.floor((1 + epoch) / STEP_SIZE))\n    lrScheduler = LearningRateScheduler(scheduler, verbose = 1)\n    #reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.25, patience = 3, min_lr = 0.0000001, verbose = 1, mode = 'auto')\n    \n    #tbCallBack = keras.callbacks.TensorBoard(log_dir = OUTPUT_PATH + 'logs/', histogram_freq = 0, write_graph = True, write_images = False)\n    \n    return [checkpoint, earlystop, lrScheduler]\n\ncallbacks_list = create_callbacks()\n\ndef build_and_fit_model(need_load = False, step_number = ''):    \n    model = create_model()\n    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(lr = LR, amsgrad = True), metrics = [\"accuracy\"])       \n    if need_load:\n        history = None\n        model.load_weights(MODELS_PATH + f'model_step_{step_number}.hdf5')\n    else:        \n        history = model.fit_generator(\n            train_generator,\n            steps_per_epoch = len(train_generator),\n            validation_data = valid_generator, \n            validation_steps = len(valid_generator),\n            epochs = EPOCHS,\n            callbacks = callbacks_list\n        )\n    return history, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history, model = build_and_fit_model(False, step_number = '1')\n\nmodel.save('../working/model_step_1.hdf5')\n\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def calc_scores():\n    return model.evaluate_generator(valid_generator, steps = len(valid_generator), verbose = 1)\n\nscores = calc_scores()\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_fig():\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_fig = range(len(acc))\n\n    plt.plot(epochs_fig, acc, 'g', label = 'Training acc')\n    plt.plot(epochs_fig, val_acc, 'r', label = 'Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs_fig, loss, 'g', label = 'Training loss')\n    plt.plot(epochs_fig, val_loss, 'r', label = 'Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()\n    \ndraw_fig()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Этап 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 8\nLR     = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = True\n# Замораживаем половину базовой модели\nfine_tune_at = len(base_model.layers) // 2\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history, model = build_and_fit_model(False, step_number = '2')\n\nmodel.save('../working/model_step_2.hdf5')\n\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = calc_scores()\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_fig()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Этап 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\nLR     = 1e-5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Размораживаем всю базовую модель\nfor layer in base_model.layers:\n    if not isinstance(layer, BatchNormalization): \n        layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history, model = build_and_fit_model(False, step_number = '3')\n\nmodel.save('../working/model_step_3.hdf5')\n\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = calc_scores()\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_fig()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!unzip '../input/sf-dl-car-classification/test.zip' -d '../working/car/'\nprint('Распаковка картинок')\nwith zipfile.ZipFile(INPUT_PATH + 'test.zip',\"r\") as z:\n    z.extractall(OUTPUT_PATH)\nprint('Распаковка завершена')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv(INPUT_PATH + 'sample-submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sub():\n    test_datagen = ImageDataGenerator(rescale = p_rescale)\n\n    test_generator = test_datagen.flow_from_dataframe(\n        dataframe = submission_df,\n        directory = OUTPUT_PATH + 'test_upload/',\n        x_col = 'Id',\n        y_col = None,\n        shuffle = False,\n        class_mode = None,\n        seed = RANDOM_SEED,\n        target_size = (IMG_SIZE, IMG_SIZE),\n        batch_size = BATCH_SIZE)\n    \n    test_generator.reset()\n    predictions = model.predict_generator(test_generator, steps=len(test_generator), verbose=1) \n    \n    predictions = np.argmax(predictions, axis = -1) #multiple categories\n    label_map = (train_generator.class_indices)\n    label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n    predictions = [label_map[k] for k in predictions]\n    \n    filenames_with_dir = test_generator.filenames\n    submission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns = ['Id', 'Category'])\n    submission['Id'] = submission['Id'].replace('test_upload/','')\n    submission.to_csv('submission.csv', index = False)    \n    submission.head()\n    print('Save submit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sub_tta():\n    test_datagen = ImageDataGenerator(\n        rescale = p_rescale,\n        zoom_range = p_zoom_range,\n        rotation_range = p_rotation_range,\n        width_shift_range = p_width_shift_range,\n        height_shift_range = p_height_shift_range,\n        shear_range = p_shear_range,    \n        horizontal_flip = True)\n\n    test_generator = test_datagen.flow_from_dataframe( \n        dataframe = submission_df,\n        directory = OUTPUT_PATH + 'test_upload/',\n        x_col = \"Id\",\n        y_col = None,\n        shuffle = False,\n        class_mode = None,\n        seed = RANDOM_SEED,\n        target_size = (IMG_SIZE, IMG_SIZE),\n        batch_size = BATCH_SIZE)\n    \n    tta_steps = 10\n    predictions = []\n\n    for i in range(tta_steps):\n        preds = model.predict_generator(test_generator, steps = len(test_generator), verbose = 1) \n        predictions.append(preds)\n\n    pred = np.mean(predictions, axis = 0)\n    \n    predictions = np.argmax(pred, axis = -1)\n    label_map = (train_generator.class_indices)\n    label_map = dict((v,k) for k,v in label_map.items())\n    predictions = [label_map[k] for k in predictions]\n    \n    filenames_with_dir = test_generator.filenames\n    submission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns = ['Id', 'Category'])\n    submission['Id'] = submission['Id'].replace('test_upload/','')\n    submission.to_csv('submission_TTA.csv', index = False)\n    submission.head()\n    print('Save submit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_tta()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}